# üí¨ Explicaci√≥n de la Implementaci√≥n del An√°lisis de Sentimientos

## üìã √çndice
1. [Introducci√≥n](#introducci√≥n)
2. [Librer√≠as Utilizadas](#librer√≠as-utilizadas)
3. [Preprocesamiento de Texto](#preprocesamiento-de-texto)
4. [Funci√≥n de Entrenamiento](#funci√≥n-de-entrenamiento)
5. [Arquitectura de la Red Neuronal](#arquitectura-de-la-red-neuronal)
6. [Funci√≥n de Predicci√≥n](#funci√≥n-de-predicci√≥n)
7. [Decisiones de Dise√±o](#decisiones-de-dise√±o)
8. [Flujo Completo](#flujo-completo)

---

## üéØ Introducci√≥n

Este m√≥dulo implementa un **clasificador de sentimientos** usando **Deep Learning** (Redes Neuronales Artificiales) con TensorFlow/Keras. Analiza comentarios de pacientes y determina si son **positivos** o **negativos**.

**Archivo:** `modelo_sentimientos.py`

**Tecnolog√≠as:**
- üß† **TensorFlow/Keras** - Red neuronal profunda
- üìä **TF-IDF** - Vectorizaci√≥n de texto
- üßπ **NLTK** - Limpieza de texto en espa√±ol
- üìà **Matplotlib/Seaborn** - Visualizaci√≥n de resultados

---

## üìö Librer√≠as Utilizadas

### **1. Procesamiento de Texto**

```python
import re
import nltk
from nltk.corpus import stopwords
```

#### **re (Regular Expressions)**
**¬øQu√© hace?**
- M√≥dulo para trabajar con expresiones regulares
- Permite buscar, reemplazar y limpiar patrones en texto

**Funciones utilizadas:**

##### `re.sub(patr√≥n, reemplazo, texto)`
```python
# Eliminar URLs
texto = re.sub(r"http\S+|www\S+", " ", texto)
# Si texto = "Visita www.ejemplo.com para m√°s"
# Resultado: "Visita   para m√°s"

# Eliminar s√≠mbolos especiales
texto = re.sub(r"[^a-z√°√©√≠√≥√∫√±√º0-9\s]", " ", texto)
# Si texto = "¬°Excelente! Muy bueno :)"
# Resultado: "Excelente  Muy bueno  "
```

**¬øPor qu√© usamos regex?**
- ‚úÖ Eficiente para limpiar grandes cantidades de texto
- ‚úÖ Flexible: un patr√≥n puede limpiar muchos casos
- ‚úÖ Estandarizado: funciona igual en cualquier texto

---

#### **NLTK (Natural Language Toolkit)**
**¬øQu√© es?**
- Librer√≠a especializada en procesamiento de lenguaje natural
- Incluye diccionarios, corpus y herramientas ling√º√≠sticas

```python
nltk.download('stopwords')
STOPWORDS = set(stopwords.words("spanish"))
```

**¬øQu√© son stopwords?**
- Palabras comunes que **no aportan significado** para clasificar sentimientos
- Ejemplos: "el", "la", "de", "que", "en", "y", etc.

**¬øPor qu√© eliminarlas?**
- ‚úÖ Reducen ruido en el modelo
- ‚úÖ El modelo se enfoca en palabras importantes
- ‚úÖ Mejora la precisi√≥n y velocidad

**Ejemplo:**
```python
STOPWORDS = {'el', 'la', 'de', 'que', 'en', 'y', 'un', 'por', ...}

texto_original = "el servicio fue muy bueno y la atenci√≥n excelente"
# Despu√©s de quitar stopwords:
# "servicio fue muy bueno atenci√≥n excelente"
```

**¬øPor qu√© usamos `set()`?**
- Las b√∫squedas en un set son O(1) (instant√°neas)
- M√°s r√°pido que buscar en una lista

---

### **2. Machine Learning**

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
```

#### **TfidfVectorizer**
**¬øQu√© hace?**
Convierte texto en n√∫meros que la red neuronal puede entender.

**TF-IDF = Term Frequency - Inverse Document Frequency**

**F√≥rmula:**
```
TF-IDF(palabra) = (Frecuencia en el documento) √ó log(Total documentos / Documentos con la palabra)
```

**Ejemplo pr√°ctico:**
```python
comentarios = [
    "excelente servicio r√°pido",
    "muy malo servicio lento",
    "excelente atenci√≥n"
]

vectorizador = TfidfVectorizer()
X = vectorizador.fit_transform(comentarios)

# Resultado (simplificado):
#              excelente  servicio  r√°pido  malo  lento  atenci√≥n
# Comentario 1:   0.58      0.42     0.58    0.0   0.0     0.0
# Comentario 2:   0.0       0.35     0.0     0.60  0.60    0.0
# Comentario 3:   0.71      0.0      0.0     0.0   0.0     0.71
```

**¬øPor qu√© TF-IDF?**
- ‚úÖ Palabras frecuentes tienen menos peso ("servicio" aparece mucho)
- ‚úÖ Palabras √∫nicas tienen m√°s peso ("excelente" es m√°s distintiva)
- ‚úÖ Funciona muy bien para clasificaci√≥n de texto

**Par√°metros en nuestro c√≥digo:**
```python
TfidfVectorizer(max_features=5000, ngram_range=(1,3), min_df=2)
```

- `max_features=5000`: Usa solo las 5000 palabras m√°s importantes
- `ngram_range=(1,3)`: Analiza palabras solas, pares y tr√≠os
- `min_df=2`: Ignora palabras que aparecen solo 1 vez

**¬øQu√© son n-grams?**
```python
texto = "muy buen servicio"

# 1-grams (palabras individuales):
["muy", "buen", "servicio"]

# 2-grams (pares):
["muy buen", "buen servicio"]

# 3-grams (tr√≠os):
["muy buen servicio"]
```

**¬øPor qu√© usar n-grams?**
- "muy bueno" tiene diferente significado que "bueno" solo
- "no recomiendo" es diferente a "recomiendo"
- Captura contexto y negaciones

---

#### **train_test_split**
```python
X_train, X_test, y_train, y_test = train_test_split(
    X.toarray(), y, test_size=0.2, random_state=42
)
```

**¬øQu√© hace?**
Divide los datos en **entrenamiento** (80%) y **prueba** (20%)

**Visualizaci√≥n:**
```
Datos totales: 100 comentarios
    ‚Üì
    ‚îú‚îÄ 80 comentarios ‚Üí Entrenamiento (el modelo aprende con estos)
    ‚îî‚îÄ 20 comentarios ‚Üí Prueba (evaluamos qu√© tan bien aprendi√≥)
```

**¬øPor qu√© dividir?**
- ‚úÖ **Entrenamiento:** El modelo aprende patrones
- ‚úÖ **Prueba:** Verificamos si funciona con datos nuevos
- ‚úÖ Evita **overfitting** (memorizar en lugar de aprender)

**Par√°metros:**
- `test_size=0.2`: 20% para prueba
- `random_state=42`: N√∫mero para reproducibilidad (siempre la misma divisi√≥n)

---

### **3. Deep Learning**

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
```

#### **TensorFlow/Keras**
**¬øQu√© es?**
- Librer√≠a de Google para crear redes neuronales
- Keras es la API de alto nivel (m√°s f√°cil de usar)

#### **Sequential**
**¬øQu√© es?**
Modelo que apila capas una tras otra (secuencialmente)

```python
modelo = Sequential()
# Capa 1
modelo.add(Dense(...))
# Capa 2
modelo.add(Dense(...))
# Capa 3
modelo.add(Dense(...))
```

#### **Dense (Capa Densa)**
**¬øQu√© es?**
Capa donde todas las neuronas est√°n conectadas entre s√≠.

```python
modelo.add(Dense(256, activation="relu", input_shape=(5000,)))
```

**Par√°metros:**
- `256`: N√∫mero de neuronas en esta capa
- `activation="relu"`: Funci√≥n de activaci√≥n (m√°s sobre esto despu√©s)
- `input_shape=(5000,)`: El input tiene 5000 caracter√≠sticas (palabras)

**Visualizaci√≥n:**
```
Input (5000 palabras)
    ‚Üì
[Neurona 1] ‚Üê‚îÄ‚îê
[Neurona 2] ‚Üê‚îÄ‚î§
[Neurona 3] ‚Üê‚îÄ‚îº‚îÄ Todas conectadas a todas
    ...      ‚îÇ
[Neurona 256]‚Üê‚îÄ‚îò
    ‚Üì
Siguiente capa
```

#### **Dropout**
**¬øQu√© hace?**
Apaga aleatoriamente algunas neuronas durante el entrenamiento.

```python
modelo.add(Dropout(0.4))  # Apaga 40% de neuronas
```

**¬øPor qu√©?**
- ‚úÖ Evita **overfitting** (memorizar)
- ‚úÖ Fuerza al modelo a aprender patrones generales
- ‚úÖ Hace la red m√°s robusta

**Analog√≠a:**
```
Estudiar para un examen:
- Sin Dropout: Memorizar las respuestas exactas
- Con Dropout: Entender los conceptos (funciona con preguntas nuevas)
```

---

### **4. Visualizaci√≥n**

```python
import matplotlib.pyplot as plt
import seaborn as sns
```

#### **Matplotlib**
Librer√≠a para crear gr√°ficos.

```python
matplotlib.use('Agg')  # Sin interfaz gr√°fica (para servidor)
```

**¬øPor qu√© `Agg`?**
- Genera im√°genes sin mostrarlas en pantalla
- Perfecto para aplicaciones web (Django)

#### **Seaborn**
Librer√≠a de visualizaci√≥n basada en Matplotlib (m√°s bonita).

```python
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
```

Crea mapas de calor con colores seg√∫n los valores.

---

## üßπ Preprocesamiento de Texto

### **Funci√≥n `limpiar_texto()`**

```python
def limpiar_texto(texto):
```

Esta funci√≥n **limpia y normaliza** el texto antes de procesarlo.

---

#### **PASO 1: Convertir a min√∫sculas**

```python
texto = texto.lower()
```

**¬øPor qu√©?**
- "Excelente" y "excelente" son la misma palabra
- Reduce vocabulario y mejora el aprendizaje

**Ejemplo:**
```python
texto = "El Servicio Fue EXCELENTE"
texto = texto.lower()
# Resultado: "el servicio fue excelente"
```

---

#### **PASO 2: Eliminar URLs**

```python
texto = re.sub(r"http\S+|www\S+", " ", texto)
```

**¬øQu√© hace?**
Elimina cualquier URL del texto.

**Patr√≥n regex explicado:**
- `http\S+`: Cualquier cosa que empiece con "http" y no tenga espacios
- `|`: O
- `www\S+`: Cualquier cosa que empiece con "www" y no tenga espacios

**Ejemplo:**
```python
texto = "Visita www.hospital.com para m√°s info"
texto = re.sub(r"http\S+|www\S+", " ", texto)
# Resultado: "Visita   para m√°s info"
```

**¬øPor qu√© eliminar URLs?**
- No aportan sentimiento
- Son ruido para el modelo

---

#### **PASO 3: Eliminar s√≠mbolos especiales**

```python
texto = re.sub(r"[^a-z√°√©√≠√≥√∫√±√º0-9\s]", " ", texto)
```

**¬øQu√© hace?**
Elimina TODO excepto letras, n√∫meros y espacios.

**Patr√≥n regex explicado:**
- `[^...]`: Negaci√≥n (todo lo que NO est√© en la lista)
- `a-z`: Letras min√∫sculas
- `√°√©√≠√≥√∫√±√º`: Letras con acentos y √±
- `0-9`: N√∫meros
- `\s`: Espacios

**Ejemplo:**
```python
texto = "¬°Excelente! Muy bueno :) #salud"
texto = re.sub(r"[^a-z√°√©√≠√≥√∫√±√º0-9\s]", " ", texto)
# Resultado: " Excelente  Muy bueno    salud"
```

**¬øPor qu√©?**
- Emojis y s√≠mbolos pueden confundir al modelo
- Estandariza el formato del texto

---

#### **PASO 4: Normalizar espacios**

```python
texto = re.sub(r"\s+", " ", texto)
```

**¬øQu√© hace?**
Reemplaza m√∫ltiples espacios consecutivos por uno solo.

**Ejemplo:**
```python
texto = "excelente    muy    bueno"
texto = re.sub(r"\s+", " ", texto)
# Resultado: "excelente muy bueno"
```

---

#### **PASO 5: Eliminar stopwords (pero conservar negaciones)**

```python
palabras = texto.split()
palabras_filtradas = [
    palabra for palabra in palabras 
    if palabra not in STOPWORDS or palabra in NEGACIONES
]
return " ".join(palabras_filtradas)
```

**¬øQu√© hace l√≠nea por l√≠nea?**

**L√≠nea 1:** Dividir texto en palabras
```python
texto = "el servicio fue muy bueno"
palabras = texto.split()
# ['el', 'servicio', 'fue', 'muy', 'bueno']
```

**L√≠nea 2-5:** List comprehension (filtro)
```python
palabras_filtradas = [
    palabra for palabra in palabras 
    if palabra not in STOPWORDS or palabra in NEGACIONES
]
```

**Desglose de la condici√≥n:**
```python
if palabra not in STOPWORDS or palabra in NEGACIONES
#  ‚Üë Condici√≥n A           ‚Üë Condici√≥n B
```

- **Condici√≥n A:** La palabra NO es stopword ‚Üí MANTENER
- **Condici√≥n B:** La palabra ES negaci√≥n ‚Üí MANTENER (especial)
- **L√≥gica `or`:** Si CUALQUIERA es True, mantener la palabra

**¬øPor qu√© conservar negaciones?**
```python
NEGACIONES = {"no", "poco", "nada", "nunca", "sin", "mal", 
              "mala", "malo", "p√©simo", "terrible", "horrible"}
```

Las negaciones **cambian completamente el sentimiento**:
- "bueno" ‚Üí POSITIVO
- "no bueno" ‚Üí NEGATIVO

**Ejemplo completo:**
```python
texto = "el servicio no fue muy bueno"
palabras = ['el', 'servicio', 'no', 'fue', 'muy', 'bueno']

# 'el' ‚Üí stopword ‚Üí ELIMINAR
# 'servicio' ‚Üí NO stopword ‚Üí MANTENER
# 'no' ‚Üí stopword PERO negaci√≥n ‚Üí MANTENER
# 'fue' ‚Üí stopword ‚Üí ELIMINAR
# 'muy' ‚Üí stopword ‚Üí ELIMINAR
# 'bueno' ‚Üí NO stopword ‚Üí MANTENER

palabras_filtradas = ['servicio', 'no', 'bueno']
```

**L√≠nea 6:** Unir las palabras de nuevo
```python
return " ".join(palabras_filtradas)
# "servicio no bueno"
```

---

### **Ejemplo Completo de `limpiar_texto()`**

```python
texto_original = "¬°El servicio fue EXCELENTE! www.hospital.com :)"

# Paso 1: Min√∫sculas
# "¬°el servicio fue excelente! www.hospital.com :)"

# Paso 2: Quitar URLs
# "¬°el servicio fue excelente!  :)"

# Paso 3: Quitar s√≠mbolos
# " el servicio fue excelente   "

# Paso 4: Normalizar espacios
# " el servicio fue excelente "

# Paso 5: Quitar stopwords
# "servicio excelente"

texto_limpio = "servicio excelente"
```

---

## üéì Funci√≥n de Entrenamiento

### **`entrenar_modelo(df)`**

Esta es la funci√≥n **m√°s importante**. Entrena la red neuronal con los comentarios.

**Par√°metro:**
- `df` (DataFrame): Tabla con columnas `texto` y `etiqueta`

---

### **PARTE 1: Preparar los Datos**

```python
# 1. Eliminar filas vac√≠as
df = df.dropna(subset=["texto", "etiqueta"]).copy()
```

**¬øQu√© hace `dropna()`?**
Elimina filas donde `texto` o `etiqueta` est√°n vac√≠as (NaN, None, "")

**¬øPor qu√© `.copy()`?**
Crea una copia para evitar modificar el DataFrame original.

---

```python
# 2. Limpiar todos los comentarios
df["texto_limpio"] = df["texto"].apply(limpiar_texto)
```

**¬øQu√© hace `apply()`?**
Aplica una funci√≥n a cada elemento de una columna.

**Equivalente con bucle:**
```python
# apply() hace esto autom√°ticamente:
for i in range(len(df)):
    df.loc[i, "texto_limpio"] = limpiar_texto(df.loc[i, "texto"])
```

**Resultado:**
```
DataFrame antes:
| texto                          | etiqueta  |
|--------------------------------|-----------|
| "¬°Excelente servicio!"         | positivo  |
| "Muy mal, no recomiendo"       | negativo  |

DataFrame despu√©s:
| texto                          | etiqueta  | texto_limpio         |
|--------------------------------|-----------|----------------------|
| "¬°Excelente servicio!"         | positivo  | "excelente servicio" |
| "Muy mal, no recomiendo"       | negativo  | "mal no recomiendo"  |
```

---

### **PARTE 2: Vectorizaci√≥n TF-IDF**

```python
vectorizador = TfidfVectorizer(max_features=5000, ngram_range=(1,3), min_df=2)
X = vectorizador.fit_transform(df["texto_limpio"])
```

**¬øQu√© hace `fit_transform()`?**
1. **`fit`:** Aprende el vocabulario (qu√© palabras existen)
2. **`transform`:** Convierte los textos en vectores num√©ricos

**Ejemplo:**
```python
textos = [
    "servicio excelente r√°pido",
    "servicio malo lento",
    "atenci√≥n excelente"
]

vectorizador = TfidfVectorizer()
X = vectorizador.fit_transform(textos)

# vocabulario aprendido: 
# ['atenci√≥n', 'excelente', 'lento', 'malo', 'r√°pido', 'servicio']

# X = matriz num√©rica (3 textos √ó 6 palabras)
```

**¬øPor qu√© `.toarray()` despu√©s?**
```python
X_train, X_test, y_train, y_test = train_test_split(
    X.toarray(), y, test_size=0.2, random_state=42
)
```

TF-IDF devuelve una matriz "sparse" (esparsa) para ahorrar memoria. La convertimos a array normal para Keras.

---

```python
joblib.dump(vectorizador, VEC_PATH)
```

**¬øQu√© hace `joblib.dump()`?**
Guarda el vectorizador en un archivo.

**¬øPor qu√© guardarlo?**
- ‚úÖ Lo necesitamos despu√©s para predecir comentarios nuevos
- ‚úÖ Debe usar el **mismo vocabulario** que en el entrenamiento
- ‚úÖ Si no lo guardamos, no podemos hacer predicciones

---

### **PARTE 3: Preparar Etiquetas**

```python
y = df["etiqueta"].map({"positivo": 1, "negativo": 0}).values
```

**¬øQu√© hace `map()`?**
Reemplaza valores seg√∫n un diccionario.

**Ejemplo:**
```python
etiquetas = ["positivo", "negativo", "positivo", "negativo"]
y = pd.Series(etiquetas).map({"positivo": 1, "negativo": 0})
# y = [1, 0, 1, 0]
```

**¬øPor qu√© convertir a n√∫meros?**
- Las redes neuronales solo entienden n√∫meros
- 1 = positivo, 0 = negativo
- La capa de salida usar√° sigmoid (0 a 1)

**¬øQu√© hace `.values`?**
Convierte la Serie de pandas a un array de NumPy (formato que Keras necesita).

---

### **PARTE 4: Dividir Datos**

```python
X_train, X_test, y_train, y_test = train_test_split(
    X.toarray(), y, test_size=0.2, random_state=42
)
```

**Resultado:**
```python
# Si tenemos 100 comentarios:
X_train: 80 comentarios vectorizados (para entrenar)
X_test:  20 comentarios vectorizados (para probar)
y_train: 80 etiquetas (1 o 0)
y_test:  20 etiquetas (1 o 0)
```

---

## üß† Arquitectura de la Red Neuronal

### **Crear el Modelo**

```python
modelo = Sequential()
```

Crea un modelo vac√≠o donde agregaremos capas.

---

### **CAPA 1: Entrada + Primera Capa Oculta**

```python
modelo.add(Dense(256, activation="relu", input_shape=(X_train.shape[1],)))
modelo.add(Dropout(0.4))
```

#### **Dense(256, activation="relu", input_shape=...)**

**Par√°metros:**
- `256`: N√∫mero de neuronas
- `activation="relu"`: Funci√≥n de activaci√≥n
- `input_shape=(5000,)`: Tama√±o del input (5000 palabras)

**¬øQu√© es ReLU?**
```
ReLU(x) = max(0, x)

Si x < 0 ‚Üí salida = 0
Si x ‚â• 0 ‚Üí salida = x
```

**Gr√°fica:**
```
  salida
    ‚Üë
    |     /
    |    /
    |   /
    |__/________‚Üí entrada
    0
```

**¬øPor qu√© ReLU?**
- ‚úÖ Simple y r√°pida
- ‚úÖ Evita el "vanishing gradient" (problema de redes profundas)
- ‚úÖ Funciona muy bien en la pr√°ctica

#### **Dropout(0.4)**
Apaga aleatoriamente 40% de las neuronas.

**Visualizaci√≥n:**
```
Entrenamiento (con Dropout):
[‚óè] [‚óã] [‚óè] [‚óã] [‚óè] [‚óè] [‚óã] [‚óè] ...
 ‚Üë   ‚Üë   ‚Üë   ‚Üë
act. off act. off  (40% apagadas)

Predicci√≥n (sin Dropout):
[‚óè] [‚óè] [‚óè] [‚óè] [‚óè] [‚óè] [‚óè] [‚óè] ...
(todas activas)
```

---

### **CAPAS 2, 3 y 4: Capas Ocultas**

```python
# Segunda capa: 128 neuronas
modelo.add(Dense(128, activation="relu"))
modelo.add(Dropout(0.4))

# Tercera capa: 64 neuronas
modelo.add(Dense(64, activation="relu"))
modelo.add(Dropout(0.3))

# Cuarta capa: 32 neuronas
modelo.add(Dense(32, activation="relu"))
```

**Patr√≥n: Pir√°mide invertida**
```
Input: 5000 caracter√≠sticas
    ‚Üì
Capa 1: 256 neuronas
    ‚Üì
Capa 2: 128 neuronas
    ‚Üì
Capa 3: 64 neuronas
    ‚Üì
Capa 4: 32 neuronas
    ‚Üì
Salida: 1 neurona
```

**¬øPor qu√© reducir gradualmente?**
- Cada capa aprende representaciones m√°s abstractas
- Capa 1: Detecta palabras y patrones simples
- Capa 2: Combina palabras (frases)
- Capa 3: Entiende contexto
- Capa 4: Representa el sentimiento general

---

### **CAPA DE SALIDA**

```python
modelo.add(Dense(1, activation="sigmoid"))
```

**Par√°metros:**
- `1`: Solo 1 neurona (clasificaci√≥n binaria: positivo/negativo)
- `activation="sigmoid"`: Funci√≥n que da valores entre 0 y 1

**¬øQu√© es Sigmoid?**
```
Sigmoid(x) = 1 / (1 + e^(-x))

Rango: 0 a 1
```

**Gr√°fica:**
```
  salida
   1 |        ___________
     |      /
   0.5|    /
     |   /
   0 |__/_____________‚Üí entrada
```

**Interpretaci√≥n:**
```
Salida = 0.8 ‚Üí 80% seguro que es POSITIVO
Salida = 0.2 ‚Üí 20% seguro que es positivo = 80% NEGATIVO
Salida = 0.5 ‚Üí No est√° seguro (umbral)
```

---

### **Arquitectura Completa Visualizada**

```
Input: [5000 palabras]
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Capa 1: 256 ReLU   ‚îÇ
‚îÇ  Dropout 40%        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Capa 2: 128 ReLU   ‚îÇ
‚îÇ  Dropout 40%        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Capa 3: 64 ReLU    ‚îÇ
‚îÇ  Dropout 30%        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Capa 4: 32 ReLU    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Salida: 1 Sigmoid  ‚îÇ
‚îÇ  (0 a 1)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì
    [Probabilidad]
```

---

### **COMPILAR EL MODELO**

```python
modelo.compile(
    optimizer="adam",
    loss="binary_crossentropy",
    metrics=["accuracy"]
)
```

#### **optimizer="adam"**
**¬øQu√© es?**
Algoritmo que ajusta los pesos de la red para minimizar el error.

**Adam = Adaptive Moment Estimation**
- Combina lo mejor de otros optimizadores
- Aprende r√°pido al inicio, se refina al final
- Es el m√°s usado actualmente

**Analog√≠a:**
```
Buscar el mejor camino en una monta√±a:
- SGD: Caminar paso a paso cuesta abajo
- Adam: Caminar cuesta abajo con "memoria" (m√°s inteligente)
```

#### **loss="binary_crossentropy"**
**¬øQu√© es?**
Funci√≥n que mide qu√© tan equivocado est√° el modelo.

**Binary Crossentropy:**
```
Loss = -[y √ó log(≈∑) + (1-y) √ó log(1-≈∑)]

y = etiqueta real (0 o 1)
≈∑ = predicci√≥n del modelo (0 a 1)
```

**Ejemplo:**
```python
# Real: positivo (y=1)
# Modelo predice: 0.9

Loss = -[1 √ó log(0.9) + 0 √ó log(0.1)]
     = -log(0.9)
     ‚âà 0.105 (bajo = bueno)

# Real: positivo (y=1)
# Modelo predice: 0.1

Loss = -[1 √ó log(0.1) + 0 √ó log(0.9)]
     = -log(0.1)
     ‚âà 2.30 (alto = malo)
```

**¬øPor qu√© esta funci√≥n?**
- ‚úÖ Penaliza mucho las predicciones muy incorrectas
- ‚úÖ Recompensa predicciones correctas y confiadas
- ‚úÖ Est√°ndar para clasificaci√≥n binaria

#### **metrics=["accuracy"]**
M√©trica adicional para monitorear durante el entrenamiento.

**Accuracy (Precisi√≥n):**
```
Accuracy = Correctas / Total

Si de 100 predicciones, 85 son correctas:
Accuracy = 85 / 100 = 0.85 = 85%
```

---

### **ENTRENAR EL MODELO**

```python
modelo.fit(
    X_train, y_train,
    epochs=20,
    batch_size=16,
    validation_split=0.2,
    verbose=1
)
```

#### **Par√°metros:**

**`epochs=20`**
- El modelo ver√° **todos** los datos 20 veces
- Cada epoch = una pasada completa por los datos

**Analog√≠a:**
```
Estudiar para un examen:
- Epoch 1: Primera lectura del material
- Epoch 2: Segunda lectura (entiendes m√°s)
- ...
- Epoch 20: Vig√©sima lectura (dominas el tema)
```

**`batch_size=16`**
- Procesa 16 comentarios a la vez antes de actualizar pesos
- M√°s peque√±o = m√°s actualizaciones = aprende detalles
- M√°s grande = menos actualizaciones = aprende patrones generales

**Proceso:**
```
Tenemos 80 comentarios de entrenamiento:

Batch 1: Comentarios 1-16   ‚Üí Calcular error ‚Üí Actualizar pesos
Batch 2: Comentarios 17-32  ‚Üí Calcular error ‚Üí Actualizar pesos
Batch 3: Comentarios 33-48  ‚Üí Calcular error ‚Üí Actualizar pesos
Batch 4: Comentarios 49-64  ‚Üí Calcular error ‚Üí Actualizar pesos
Batch 5: Comentarios 65-80  ‚Üí Calcular error ‚Üí Actualizar pesos

= 1 epoch completo
```

**`validation_split=0.2`**
- Usa 20% de los datos de entrenamiento para validaci√≥n
- **Validaci√≥n:** Evaluar el modelo durante el entrenamiento (no despu√©s)

**Divisi√≥n:**
```
Datos originales: 100 comentarios
    ‚Üì
Entrenamiento: 80 comentarios
    ‚îú‚îÄ Para entrenar: 64 comentarios (80%)
    ‚îî‚îÄ Para validar: 16 comentarios (20%)
Prueba: 20 comentarios
```

**¬øPor qu√© validaci√≥n?**
- Detectar **overfitting** temprano
- Ver si el modelo generaliza bien
- Decidir cu√°ndo parar el entrenamiento

**`verbose=1`**
- Muestra progreso en la consola
- verbose=0: sin output
- verbose=1: barra de progreso
- verbose=2: una l√≠nea por epoch

**Ejemplo de output:**
```
Epoch 1/20
5/5 [==============================] - 2s - loss: 0.6234 - accuracy: 0.7000 - val_loss: 0.5892 - val_accuracy: 0.7500
Epoch 2/20
5/5 [==============================] - 1s - loss: 0.5421 - accuracy: 0.7625 - val_loss: 0.5123 - val_accuracy: 0.8125
...
```

---

### **GUARDAR EL MODELO**

```python
modelo.save(MODEL_PATH)
```

**¬øQu√© hace?**
Guarda toda la red neuronal en un archivo `.h5`:
- Arquitectura (capas, neuronas)
- Pesos entrenados
- Configuraci√≥n del optimizador

**¬øPor qu√© guardar?**
- No queremos entrenar cada vez que hacemos una predicci√≥n
- Entrenar toma tiempo (minutos/horas)
- Predecir con modelo guardado es instant√°neo

---

### **EVALUAR EL MODELO**

```python
perdida, precision = modelo.evaluate(X_test, y_test, verbose=0)
```

**¬øQu√© hace `evaluate()`?**
Prueba el modelo con datos que **nunca** ha visto.

**Retorna:**
- `perdida`: Valor de la funci√≥n de p√©rdida (loss)
- `precision`: Accuracy (% de aciertos)

**Ejemplo:**
```python
# 20 comentarios de prueba
# Modelo acierta 17

precision = 17 / 20 = 0.85 = 85%
```

---

### **GENERAR PREDICCIONES**

```python
y_pred_prob = modelo.predict(X_test, verbose=0)
y_pred = (y_pred_prob > 0.5).astype(int).flatten()
```

**L√≠nea 1:** Obtener probabilidades
```python
y_pred_prob = [[0.85], [0.23], [0.91], [0.12], ...]
# Cada valor es la probabilidad de ser POSITIVO
```

**L√≠nea 2:** Convertir a etiquetas (0 o 1)
```python
y_pred_prob > 0.5
# [True, False, True, False, ...]

.astype(int)
# [1, 0, 1, 0, ...]

.flatten()
# Aplanar array de [[1], [0], [1]] a [1, 0, 1]
```

**Umbral 0.5:**
```
Probabilidad ‚â• 0.5 ‚Üí Positivo (1)
Probabilidad < 0.5 ‚Üí Negativo (0)
```

---

### **MATRIZ DE CONFUSI√ìN**

```python
cm = confusion_matrix(y_test, y_pred)
```

**¬øQu√© es?**
Tabla que muestra aciertos y errores del modelo.

**Estructura:**
```
                Predicci√≥n
                Neg   Pos
Real  Neg  [  TN  |  FP  ]
      Pos  [  FN  |  TP  ]

TN = True Negative (correcto: predijo negativo, era negativo)
FP = False Positive (error: predijo positivo, era negativo)
FN = False Negative (error: predijo negativo, era positivo)
TP = True Positive (correcto: predijo positivo, era positivo)
```

**Ejemplo:**
```python
cm = [[8, 2],
      [1, 9]]

# 8 negativos correctos
# 2 falsos positivos (dijo positivo, era negativo)
# 1 falso negativo (dijo negativo, era positivo)
# 9 positivos correctos
```

**M√©tricas calculadas:**
```python
tn, fp, fn, tp = cm.ravel()

accuracy = (tp + tn) / (tp + tn + fp + fn)
# (9 + 8) / 20 = 17/20 = 85%

precision = tp / (tp + fp)
# 9 / (9 + 2) = 9/11 = 82%

recall = tp / (tp + fn)
# 9 / (9 + 1) = 9/10 = 90%

f1_score = 2 √ó (precision √ó recall) / (precision + recall)
# 2 √ó (0.82 √ó 0.90) / (0.82 + 0.90) = 86%
```

---

### **VISUALIZACI√ìN**

```python
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
```

**Par√°metros:**
- `annot=True`: Mostrar n√∫meros en cada celda
- `fmt='d'`: Formato entero (sin decimales)
- `cmap='Blues'`: Escala de colores azules

**Guardar en memoria (no en archivo):**
```python
buffer_cm = BytesIO()
plt.savefig(buffer_cm, format='png')
buffer_cm.seek(0)
imagen_cm = base64.b64encode(buffer_cm.read()).decode('utf-8')
```

**¬øPor qu√© `BytesIO`?**
- Guarda la imagen en memoria (RAM)
- No crea archivos en el disco
- Perfecto para web (enviar imagen directamente)

**¬øPor qu√© `base64.b64encode`?**
- Convierte la imagen binaria a texto
- Se puede insertar directamente en HTML
- `<img src="data:image/png;base64,{imagen_cm}">`

---

## üîÆ Funci√≥n de Predicci√≥n

### **`cargar_modelo()`**

```python
def cargar_modelo():
    if not os.path.exists(MODEL_PATH) or not os.path.exists(VEC_PATH):
        return None, None
    
    modelo = tf.keras.models.load_model(MODEL_PATH)
    vectorizador = joblib.load(VEC_PATH)
    
    return modelo, vectorizador
```

**¬øQu√© hace?**
1. Verifica que existan los archivos guardados
2. Carga el modelo y vectorizador
3. Los retorna para usarlos

**¬øPor qu√© verificar primero?**
- Si no existe el archivo, `load()` dar√≠a error
- Mejor retornar `None` y manejar el error elegantemente

---

### **`predecir(texto)`**

```python
def predecir(texto):
```

Esta funci√≥n **predice el sentimiento** de un comentario nuevo.

---

#### **PASO 1: Cargar el Modelo**

```python
modelo, vectorizador = cargar_modelo()

if modelo is None:
    return {"ok": False, "error": "Modelo no entrenado a√∫n."}
```

Si no hay modelo, retornar error.

---

#### **PASO 2: Limpiar el Texto**

```python
texto_limpio = limpiar_texto(texto)
```

Aplicar el mismo preprocesamiento que en el entrenamiento.

**Ejemplo:**
```python
texto = "¬°El servicio fue EXCELENTE!"
texto_limpio = "servicio excelente"
```

---

#### **PASO 3: Vectorizar**

```python
X = vectorizador.transform([texto_limpio]).toarray()
```

**¬øPor qu√© `transform()` y no `fit_transform()`?**
- `fit_transform()`: Aprende vocabulario + transforma (entrenamiento)
- `transform()`: Solo transforma usando vocabulario ya aprendido (predicci√≥n)

**¬øPor qu√© `[texto_limpio]` con corchetes?**
- `transform()` espera una lista de textos
- Aunque sea uno solo, debe estar en lista: `["texto"]`

**Resultado:**
```python
X = [[0.0, 0.58, 0.0, 0.71, ..., 0.0]]
# Array con 5000 valores (uno por palabra del vocabulario)
```

---

#### **PASO 4: Predecir**

```python
probabilidad = modelo.predict(X, verbose=0)[0][0]
```

**Desglose:**
```python
resultado = modelo.predict(X, verbose=0)
# resultado = [[0.85]]  (array 2D)

[0]
# [0.85]  (primer elemento, array 1D)

[0]
# 0.85  (valor escalar)
```

**¬øPor qu√© `verbose=0`?**
- No mostrar mensajes en consola durante la predicci√≥n

---

#### **PASO 5: Ajuste con Palabras Clave Negativas**

```python
palabras_texto = texto.lower().split()
palabras_negativas_fuertes = ["p√©simo", "p√©sima", "horrible", "terrible", 
                               "fatal", "malo", "mala", "malos", "malas"]

tiene_negacion_fuerte = any(palabra in palabras_negativas_fuertes 
                            for palabra in palabras_texto)
```

**¬øQu√© hace `any()`?**
Retorna `True` si **al menos una** palabra est√° en la lista.

**Equivalente con bucle:**
```python
tiene_negacion_fuerte = False
for palabra in palabras_texto:
    if palabra in palabras_negativas_fuertes:
        tiene_negacion_fuerte = True
        break
```

---

```python
if tiene_negacion_fuerte and probabilidad < 0.65:
    etiqueta = "negativo"
    probabilidad = max(0.3, probabilidad - 0.2)
elif probabilidad >= 0.5:
    etiqueta = "positivo"
else:
    etiqueta = "negativo"
```

**L√≥gica:**

**Caso 1:** Tiene palabra muy negativa Y probabilidad dudosa
```python
texto = "El servicio fue p√©simo"
probabilidad = 0.55  # Cerca del umbral

# Ajuste:
etiqueta = "negativo"
probabilidad = 0.55 - 0.2 = 0.35
```

**Caso 2:** Probabilidad ‚â• 0.5
```python
probabilidad = 0.8
etiqueta = "positivo"
```

**Caso 3:** Probabilidad < 0.5
```python
probabilidad = 0.3
etiqueta = "negativo"
```

**¬øPor qu√© este ajuste?**
- Mejora la detecci√≥n de comentarios muy negativos
- A veces el modelo no detecta bien palabras extremas
- Es una **heur√≠stica adicional** que complementa la red neuronal

---

#### **PASO 6: Retornar Resultado**

```python
return {
    "ok": True,
    "etiqueta": etiqueta,
    "confianza": float(probabilidad)
}
```

**Ejemplo de retorno:**
```python
{
    "ok": True,
    "etiqueta": "positivo",
    "confianza": 0.85
}
```

---

## üéì Decisiones de Dise√±o

### **Tabla Resumen**

| Decisi√≥n | Alternativa | ¬øPor qu√© esto? |
|----------|-------------|----------------|
| **Red neuronal profunda (4 capas)** | Red simple (1-2 capas) | ‚úÖ Mejor para captar patrones complejos<br>‚úÖ Entiende contexto y negaciones |
| **TF-IDF en lugar de Word2Vec** | Word embeddings | ‚úÖ M√°s simple de implementar<br>‚úÖ Funciona bien con pocos datos |
| **N-grams (1,3)** | Solo palabras individuales | ‚úÖ Captura frases ("muy bueno")<br>‚úÖ Detecta negaciones ("no recomiendo") |
| **Dropout 40%** | Sin dropout o menos | ‚úÖ Evita overfitting<br>‚úÖ Dataset relativamente peque√±o |
| **ReLU en capas ocultas** | Sigmoid o Tanh | ‚úÖ M√°s r√°pida<br>‚úÖ Evita vanishing gradient |
| **Sigmoid en salida** | Softmax | ‚úÖ Clasificaci√≥n binaria<br>‚úÖ Output entre 0 y 1 |
| **Adam optimizer** | SGD | ‚úÖ Aprende m√°s r√°pido<br>‚úÖ Auto-ajusta learning rate |
| **20 epochs** | 10 o 50 | ‚úÖ Balance entre tiempo y precisi√≥n<br>‚úÖ Evita overfitting |
| **Batch size 16** | 32 o 64 | ‚úÖ Bueno para datasets peque√±os<br>‚úÖ M√°s actualizaciones de pesos |
| **Conservar negaciones** | Eliminar todas las stopwords | ‚úÖ "no bueno" ‚â† "bueno"<br>‚úÖ Cr√≠tico para sentimientos |
| **Ajuste heur√≠stico final** | Solo red neuronal | ‚úÖ Mejora detecci√≥n de extremos<br>‚úÖ Compensa limitaciones del modelo |

---

### **Arquitectura: ¬øPor qu√© esta estructura?**

```
Input (5000) ‚Üí 256 ‚Üí 128 ‚Üí 64 ‚Üí 32 ‚Üí 1 (Output)
```

**Pir√°mide invertida:**
- **Inicio (256):** Capacidad para aprender muchos patrones
- **Medio (128, 64):** Combina patrones en representaciones abstractas
- **Final (32):** Representaci√≥n compacta del sentimiento
- **Salida (1):** Decisi√≥n final (positivo/negativo)

**Alternativas descartadas:**
```
# Muy simple (poco poder de aprendizaje)
5000 ‚Üí 64 ‚Üí 1

# Muy compleja (overfitting con pocos datos)
5000 ‚Üí 512 ‚Üí 512 ‚Üí 256 ‚Üí 256 ‚Üí 128 ‚Üí 1
```

---

### **Complejidad Computacional**

#### **Entrenamiento:**
- **Tiempo:** ~2-5 minutos (depende del hardware)
- **Memoria:** ~500 MB RAM
- **Operaciones por epoch:** Millones de multiplicaciones matriciales

#### **Predicci√≥n:**
- **Tiempo:** <100 ms por comentario
- **Memoria:** ~200 MB RAM (modelo cargado)

---

## üîÑ Flujo Completo del C√≥digo

### **Diagrama de Flujo: Entrenamiento**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ entrenar_modelo(df)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Limpiar datos            ‚îÇ
‚îÇ    - Eliminar vac√≠os        ‚îÇ
‚îÇ    - Aplicar limpiar_texto()‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Vectorizaci√≥n TF-IDF     ‚îÇ
‚îÇ    - fit_transform()        ‚îÇ
‚îÇ    - Guardar vectorizador   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. Preparar etiquetas       ‚îÇ
‚îÇ    - positivo ‚Üí 1           ‚îÇ
‚îÇ    - negativo ‚Üí 0           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. Dividir datos            ‚îÇ
‚îÇ    - 80% entrenamiento      ‚îÇ
‚îÇ    - 20% prueba             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. Crear red neuronal       ‚îÇ
‚îÇ    - 4 capas Dense          ‚îÇ
‚îÇ    - Dropout                ‚îÇ
‚îÇ    - Compilar               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 6. Entrenar (20 epochs)     ‚îÇ
‚îÇ    - Ajustar pesos          ‚îÇ
‚îÇ    - Validaci√≥n             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 7. Guardar modelo           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 8. Evaluar y generar        ‚îÇ
‚îÇ    gr√°ficos                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ RETORNAR resultados         ‚îÇ
‚îÇ {accuracy, gr√°ficos, ...}   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### **Diagrama de Flujo: Predicci√≥n**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ predecir(texto)             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Cargar modelo y          ‚îÇ
‚îÇ    vectorizador             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ ¬øExiste? ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   NO             S√ç
    ‚îÇ              ‚îÇ
    ‚Üì              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ERROR   ‚îÇ  ‚îÇ 2. Limpiar texto‚îÇ
‚îÇ Retornar‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
                      ‚Üì
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ 3. Vectorizar        ‚îÇ
           ‚îÇ    (transform)       ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚Üì
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ 4. Predecir con      ‚îÇ
           ‚îÇ    modelo.predict()  ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚Üì
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ 5. Ajustar con       ‚îÇ
           ‚îÇ    heur√≠stica        ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
                      ‚Üì
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ 6. RETORNAR          ‚îÇ
           ‚îÇ {etiqueta, confianza}‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üé§ Puntos Clave para Presentar

### **1. Preprocesamiento**
> "Antes de entrenar, limpiamos el texto: eliminamos s√≠mbolos, URLs y stopwords. **Pero conservamos negaciones** porque cambian completamente el sentimiento."

### **2. Vectorizaci√≥n**
> "Las redes neuronales no entienden palabras, solo n√∫meros. Usamos **TF-IDF** para convertir cada comentario en un vector de 5000 n√∫meros que representa la importancia de cada palabra."

### **3. Arquitectura**
> "Dise√±amos una red neuronal **profunda con 4 capas ocultas** que aprende patrones cada vez m√°s abstractos: desde palabras individuales hasta el sentimiento general del comentario."

### **4. Regularizaci√≥n**
> "Usamos **Dropout** para evitar que el modelo memorice. Apagamos aleatoriamente 40% de las neuronas durante el entrenamiento, forz√°ndolo a aprender patrones generales que funcionen con comentarios nuevos."

### **5. Entrenamiento**
> "El modelo ve los datos **20 veces (epochs)**, aprendiendo m√°s en cada pasada. Usamos 80% para entrenar y 20% para probar qu√© tan bien generaliza."

### **6. Predicci√≥n**
> "Para predecir un comentario nuevo, lo limpiamos, lo vectorizamos con el **mismo vocabulario** del entrenamiento, y la red neuronal nos da una probabilidad entre 0 y 1. Mayor a 0.5 es positivo, menor es negativo."

---

## ‚úÖ Checklist para Entender el C√≥digo

- ‚úÖ Entiendo qu√© son las stopwords y por qu√© se eliminan
- ‚úÖ S√© qu√© es TF-IDF y c√≥mo convierte texto en n√∫meros
- ‚úÖ Entiendo la arquitectura de la red neuronal (capas, neuronas)
- ‚úÖ S√© qu√© hacen ReLU y Sigmoid
- ‚úÖ Entiendo qu√© es Dropout y por qu√© se usa
- ‚úÖ S√© c√≥mo se divide en entrenamiento/validaci√≥n/prueba
- ‚úÖ Entiendo qu√© hace Adam optimizer
- ‚úÖ S√© interpretar la matriz de confusi√≥n
- ‚úÖ Entiendo el flujo completo: entrenar ‚Üí guardar ‚Üí cargar ‚Üí predecir

---

**Autor**: Sistema de Salud - M√≥dulo de An√°lisis de Sentimientos  
**Documento**: Explicaci√≥n T√©cnica de la Implementaci√≥n  
**Fecha**: Noviembre 2025  
**Versi√≥n**: 1.0
